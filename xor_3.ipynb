{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the outputs of logic gates into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B  NEW\n",
      "1  0  0    0\n",
      "2  0  1    1\n",
      "3  1  0    1\n",
      "4  1  1    0\n",
      "5  2  0    0\n",
      "6  2  1    1\n"
     ]
    }
   ],
   "source": [
    "logic_gates = {'A':[0,0,1,1,2,2],'B':[0,1,0,1,0,1],'NEW':[0,1,1,0,0,1]}\n",
    "table = pd.DataFrame(data = logic_gates,columns=['A','B','NEW'],index=[1,2,3,4,5,6])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining and initializing the weights and biases for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_w = tf.Variable(tf.random_normal([2,2]))\n",
    "layer_1_b = tf.Variable(tf.random_normal([2,1]))\n",
    "layer_2_w = tf.Variable(tf.random_normal([1,2]))\n",
    "layer_2_b = tf.Variable(.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 0\n",
    "for a,b,c in zip(table['A'],table['B'],table['NEW']):\n",
    "    output_l1 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1_w,[[float(a)],[float(b)]]),layer_1_b))\n",
    "    output_l2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2_w,output_l1),layer_2_b))\n",
    "    err += (output_l2-c)**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.GradientDescentOptimizer(learning_rate=.1)\n",
    "tran = opt.minimize(err)\n",
    "ex = np.linspace(-.1,1.1,100)\n",
    "ey = np.linspace(-.1,1.1,100)\n",
    "ez_1 = np.zeros([100,100],float)\n",
    "ez_2 = np.zeros([100,100],float)\n",
    "ez_3 = np.zeros([100,100],float)\n",
    "params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    training_steps = 100000\n",
    "    for i in range(training_steps):\n",
    "        sess.run(tran)\n",
    "    print(\"error:{0}\".format(sess.run(err)))\n",
    "    for a,b,c in zip(table['A'],table['B'],table['NEW']):\n",
    "        output_l1 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1_w,[[float(a)],[float(b)]]),layer_1_b))\n",
    "        output_l2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2_w,output_l1),layer_2_b))\n",
    "        print('A:{0} B:{1} C:{2} pred:{3}'.format(a,b,c,sess.run(output_l2)))\n",
    "    params.append([sess.run([layer_1_w,layer_1_b,layer_2_w,layer_2_b])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS OF PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the output for each point in plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    l1 = sigmoid(np.dot(params[0][0][0],x)+params[0][0][1])\n",
    "    l2 = sigmoid(np.dot(params[0][0][2],l1)+params[0][0][3])\n",
    "    Neuron = {'N1':l1[0][0],'N2':l1[1][0],'N3':l2[0][0]}\n",
    "    return Neuron['N1'],Neuron['N2'],Neuron['N3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        ez_1[i][j],ez_2[i][j],ez_3[i][j] = predict([[ex[i]],[ex[j]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's dig deeper and ask \"What are the individual outputs of each neuron?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ez_1,interpolation='nearest' , cmap=plt.cm.ocean, extent=(-.1,2.1,1.1,-.1))\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.plot([0,1,2],[0,1,0],'ro',label='0')\n",
    "plt.plot([0,1,2],[1,0,1],'bo',label='1')\n",
    "plt.colorbar()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron 2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ez_2,interpolation='nearest' , cmap=plt.cm.ocean,  extent=(-.1,2.1,1.1,-.1))\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.plot([0,1,2],[0,1,0],'ro',label='0')\n",
    "plt.plot([0,1,2],[1,0,1],'bo',label='1')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron 3 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ez_3, interpolation='nearest' ,cmap=plt.cm.ocean,  extent=(-.1,2.1,1.1,-.1))\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.rcParams['xtick.top'] = plt.rcParams['xtick.labeltop'] = True\n",
    "plt.plot([0,1,2],[0,1,0],'ro',label='0')\n",
    "plt.plot([0,1,2],[1,0,1],'bo',label='1')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLVED ! \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph clearly shows that the points have been classified correctly. Hurray!\n",
    "<n>\n",
    "Outputs of each neuron has been rounded off and printed in the next cell for xor gate inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b,c in zip(table['A'],table['B'],table['NEW']):\n",
    "    node1,node2,node3 = predict([[float(a)],[float(b)]])\n",
    "    print(\"A:{0} | B:{1} | C:{2}| neuron_1 -> {3} | neuron_2 -> {4}  | neuron_3 -> {5}\".format(a,b,c,round(node1),round(node2),round(node3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see why the outputs of neuron_1 and neuron_2 combine with neuron_3 to produce the right classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SIGMOID[(w1 x n1)+(w2 x n2)] + b = output\")\n",
    "for a,b in zip(table['A'],table['B']):\n",
    "    node1,node2,node3 = predict([[float(a)],[float(b)]])\n",
    "    print('SIGMOID[({0} x {3}) + ({1} x {4}) + {2}] ~ {5} '.format(params[0][0][2][0][0],params[0][0][2][0][1],params[0][0][3],round(node1),round(node2),round(node3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that neuron_1 and neuron_2 simply approximately divides the plane into two linearly just like a peceptron with a sigmoid activation function. Neuron_3 cleverly adjusts itself. It simply multiplies output of neuron_1 with ~9 and output of neuron_2 by ~ -9 adds them and then subtracts ~4 from it .\n",
    "\n",
    "## Neuron_1 acts like NAND gate\n",
    "## Neuron_2 acts like NOR gate\n",
    "## What does Neuron_3 acts like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b in zip(table['A'],table['B']):\n",
    "    out_3 = sigmoid(params[0][0][2][0][0]*float(a) + params[0][0][2][0][1]*float(b) + params[0][0][3])\n",
    "    print(\"A:{0} | B:{1} | Neuron_3:{2}\".format(a,b,round(out_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron_3 doesn't act like any logic gate we know. Infact It has created a problem specific gate to deal only with the inputs it receive. If you observe the inputs Neuron_3 receives you will see why it behaves in such a peculiar fashion. \n",
    "\n",
    "### |---Neuron_1----|---Neuron_2---|---Neuron_3--|\n",
    "### |---------1----------|---------1---------|--------0---------|\n",
    "### |---------1----------|---------0---------|--------1---------|\n",
    "### |---------1----------|---------0---------|--------1---------|\n",
    "### |---------0----------|---------0---------|--------0---------|\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the above are the only input it has to deal with. Though each perceptron separates the plane linearly its the architecture that gives it the flexibility to learn non-linear boundaries. One cold connect a million perceptrons end to end and still not solve the problem without the right architecture the problem still may not be solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Though I started out with the idea of finding a way to get the right architecture for a given problem, I feel that the results that I got does not suggest  much in that direction. Here Neuron_1 and Neuron_2 acts as NAND and NOR gates. I don't think this is a unique solution there might be so many problem specific gates that our neurons could have developed to solve the problem. For example the neurons could simply reverse the order and the third neuron could switch the weights and still the solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am pretty sure that running this code might change the whole parameters of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feel free to play with the code!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
